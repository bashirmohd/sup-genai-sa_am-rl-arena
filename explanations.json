{
  "algorithms": {
    "q-learning": {
      "title": "Q-Learning (Off-Policy TD)",
      "text": "<p><strong>Goal:</strong> Learns the optimal action-value function \\(Q^*(s,a)\\), which represents the maximum expected future reward for taking action \\(a\\) in state \\(s\\) and following the optimal policy thereafter.</p><p><strong>Representation:</strong> Maintains a table (Q-table) mapping each state-action pair \\((s,a)\\) to its estimated Q-value. This is often implemented as a matrix where rows represent states and columns represent actions.</p><p><strong>How it works:</strong> It updates the Q-value for the current state-action pair in the Q-table based on the reward received and the <em>maximum</em> Q-value of the <em>next</em> state (greedy approach). It's \"off-policy\" because the action used for the update (the best next action) might be different from the action the agent actually takes next (which could be exploratory).</p><p><strong>Update Rule:</strong></p>\\[ Q(s,a) \\leftarrow Q(s,a) + \\alpha [R + \\gamma \\max_{a'} Q(s',a') - Q(s,a)] \\]<p>Where \\(\\alpha\\) is the <em>Learning Rate</em> (controls how much new information overrides the old estimate, typically 0 < \\(\\alpha\\) ≤ 1) and \\(\\gamma\\) is the <em>Discount Factor</em> (determines the importance of future rewards; 0 means only immediate reward matters, close to 1 means future rewards are highly valued, typically 0 ≤ \\(\\gamma\\) < 1).</p>"
    },
    "sarsa": {
      "title": "SARSA (On-Policy TD)",
      "text": "<p><strong>Goal:</strong> Learns the action-value function \\(Q(s,a)\\) for the <em>current policy</em> being followed by the agent (including exploration).</p><p><strong>Representation:</strong> Uses a Q-table, mapping state-action pairs \\((s,a)\\) to their Q-values, similar to Q-Learning. This is often implemented as a matrix where rows represent states and columns represent actions.</p><p><strong>How it works:</strong> It updates the Q-value in the table based on the reward received and the Q-value of the <em>actual</em> state-action pair \\((s', a')\\) that comes next according to the current policy. It's \"on-policy\" because the update uses the same policy that generated the action.</p><p><strong>Update Rule:</strong></p>\\[ Q(s,a) \\leftarrow Q(s,a) + \\alpha [R + \\gamma Q(s',a') - Q(s,a)] \\]<p>Where \\(a'\\) is the action actually taken in state \\(s'\\). \\(\\alpha\\) is the <em>Learning Rate</em> (step size for updates) and \\(\\gamma\\) is the <em>Discount Factor</em> (importance of future rewards). SARSA stands for State-Action-Reward-State-Action.</p>"
    },
    "expected-sarsa": {
      "title": "Expected SARSA (On-Policy TD)",
      "text": "<p><strong>Goal:</strong> Learns the action-value function \\(Q(s,a)\\) for the <em>current policy</em>, similar to SARSA, but often with lower variance.</p><p><strong>Representation:</strong> Maintains a Q-table mapping state-action pairs \\((s,a)\\) to Q-values, typically implemented as a state-by-action matrix.</p><p><strong>How it works:</strong> Instead of using the Q-value of the single next action taken \\((a')\\), it uses the <em>expected</em> Q-value of the next state. This expectation is calculated by averaging the Q-values (from the Q-table) of all possible next actions, weighted by their probabilities \\(\\pi(a'|s')\\) under the current policy.</p><p><strong>Update Rule:</strong></p>\\[ Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ R + \\gamma \\sum_{a'} \\pi(a'|s') Q(s',a') - Q(s,a) \\right] \\]<p>Where \\(\\alpha\\) is the <em>Learning Rate</em> (step size for updates) and \\(\\gamma\\) is the <em>Discount Factor</em> (importance of future rewards). This avoids the randomness introduced by sampling \\(a'\\) in the update, leading to smoother learning.</p>"
    },
    "monte-carlo": {
      "title": "Monte Carlo (MC) Control",
      "text": "<p><strong>Goal:</strong> Learns the action-value function \\(Q(s,a)\\) by averaging the returns (total discounted rewards) observed after visiting state-action pairs over many complete episodes.</p><p><strong>Representation:</strong> Uses a Q-table mapping state-action pairs \\((s,a)\\) to their estimated average return (Q-value), often implemented as a state-by-action matrix. It also typically requires a way to store the returns observed for each state-action pair before averaging (e.g., a list of returns per pair).</p><p><strong>How it works:</strong> The agent runs through an entire episode. Only after the episode is finished does it go back through the steps taken. For each state-action pair \\((s,a)\\) visited at timestep \\(t\\), it calculates the total discounted reward \\(G_t = R_{t+1} + \\gamma R_{t+2} + \\dots\\) received from that point until the end of the episode. It then updates the entry for \\((s,a)\\) in the Q-table to be closer to the average \\(G_t\\) observed for that pair.</p><p><strong>Update Rule (Simplified, Every-Visit):</strong></p>\\[ Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha [G_t - Q(s_t,a_t)] \\]<p>Where \\(\\alpha\\) is the <em>Learning Rate</em>, controlling how much the latest episode's return updates the average, and \\(\\gamma\\) (the <em>Discount Factor</em> used in \\(G_t\\)) determines the present value of future rewards. MC methods don't require a model of the environment and learn directly from experience, but they can only be applied to episodic tasks and updates only happen at the end of episodes.</p>"
    },
    "actor-critic": {
      "title": "Actor-Critic (Advantage Actor-Critic)",
      "text": "<p><strong>Goal:</strong> Combines policy learning (Actor) and value learning (Critic) to find an optimal policy.</p><p><strong>Representation:</strong><ul><li>The <strong>Critic</strong> maintains a representation of the state-value function \\(V(s)\\). For tabular cases, this is typically a table or array mapping each state \\(s\\) to its estimated value.</li><li>The <strong>Actor</strong> maintains a representation of the policy \\(\\pi(a|s)\\), often parameterized by preferences \\(h(s,a)\\). In tabular settings, this could be a table mapping state-action pairs to preference scores, from which action probabilities are derived (e.g., via softmax).</li></ul></p><p><strong>How it works:</strong><ul><li>The <strong>Critic</strong> learns the state-value function \\(V(s)\\) using Temporal Difference (TD) learning.</li><li>The <strong>Actor</strong> learns the policy \\(\\pi(a|s)\\).</li><li>The Critic calculates the <strong>TD Error</strong> (or Advantage): \\(\\delta_t = R_{t+1} + \\gamma V(s_{t+1}) - V(s_t)\\). Here, \\(\\gamma\\) is the <em>Discount Factor</em>, balancing immediate reward and discounted future value.</li><li>The Critic updates its value estimate \\(V(s_t)\\) using \\(\\delta_t\\): \\(V(s_t) \\leftarrow V(s_t) + \\alpha_c \\delta_t\\). \\(\\alpha_c\\) is the critic's <em>Learning Rate</em>.</li><li>The Actor updates its policy parameters (e.g., preferences \\(h(s_t, a_t)\\)) using \\(\\delta_t\\) and the policy gradient (often with its own learning rate \\(\\alpha_a\\)), essentially increasing the preference for actions that led to positive TD errors and decreasing it for those leading to negative errors.</li></ul></p><p>This allows the agent to learn online (like TD methods) while directly optimizing the policy (like policy gradient methods).</p>"
    },
    "sr": {
      "title": "Successor Representation (SR)",
      "text": "<p><strong>Goal:</strong> Learns a predictive representation of state occupancy (Successor Representation, \\(M\\)) and reward weights (\\(w\\)) to compute state values \\(V\\) and action values \\(Q\\). It decouples environment dynamics/transitions from rewards.</p><p><strong>Representation:</strong><ul><li>The <strong>Successor Representation (SR)</strong> is stored as a matrix \\(M\\) where rows represent the starting state \\(s\\) and columns represent the future state \\(s'\\). \\(M(s, s')\\) estimates the expected discounted future occupancy of state \\(s'\\) starting from state \\(s\\).</li><li>The <strong>Reward Weights</strong> are stored as a vector \\(w\\) where each element \\(w(s')\\) represents the learned expected immediate reward for entering state \\(s'\\).</li></ul></p><p><strong>How it works:</strong><ul><li>The SR matrix \\(M\\) is learned via TD updates reflecting the policy's transitions.</li><li>The reward weight vector \\(w\\) is learned via TD updates based on received rewards.</li><li>The <strong>State Value \\(V(s)\\)</strong> is computed by combining \\(M\\) and \\(w\\): \\( V(s) = \\sum_{s'} M(s, s') w(s') \\) (matrix-vector multiplication).</li><li>The <strong>Action Value \\(Q(s, a)\\)</strong> can be estimated based on the expected next state \\(s'\\) after taking action \\(a\\) in state \\(s\\): \\( Q(s, a) \\approx w(s') + \\gamma V(s') \\) (using the learned \\(w(s')\\) as the immediate reward estimate and the computed \\(V(s')\\)).</li><li>Action selection uses these computed Q-values with standard exploration strategies (ε-Greedy, Softmax).</li></ul></p><p><strong>Update Rules:</strong></p><p>1. Weight Update (after \\(s \\rightarrow s'\\) with reward \\(R\\)):</p>\\[ w(s') \\leftarrow w(s') + \\alpha_w [R - w(s')] \\]<p>2. SR Update (for all \\(s_{prime}\\) in the grid):</p>\\[ M(s, s_{prime}) \\leftarrow M(s, s_{prime}) + \\alpha_M [ \\mathbb{I}(s' = s_{prime}) + \\gamma M(s', s_{prime}) - M(s, s_{prime}) ] \\]<p>Where \\(\\mathbb{I}(\\cdot)\\) is the indicator function. \\(\\alpha_w\\) is the <em>Learning Rate</em> for reward weights, \\(\\alpha_M\\) is the <em>Learning Rate</em> for the SR matrix, and \\(\\gamma\\) is the <em>Discount Factor</em> controlling how much future state occupancies contribute to the current state's representation. Note that updating \\(M\\) requires iterating over all possible future states \\(s_{prime}\\) in each step, making it computationally more intensive than standard TD methods for large state spaces.</p>"
    }
  },
  "strategies": {
    "epsilon-greedy": {
      "title": "ε-Greedy Exploration",
      "text": "<p><strong>Idea:</strong> Mostly act greedily (exploit), but sometimes explore randomly.</p><p><strong>How it works:</strong> With probability \\(1 - \\epsilon\\), choose the action with the highest estimated Q-value (\\(\\arg\\max_a Q(s,a)\\)). With probability \\(\\epsilon\\), choose a random action. The Exploration Rate \\(\\epsilon\\) controls the trade-off. Higher \\(\\epsilon\\) means more exploration.</p>"
    },
    "softmax": {
      "title": "Softmax (Boltzmann) Exploration",
      "text": "<p><strong>Idea:</strong> Choose actions probabilistically based on their Q-values. Higher Q-values mean higher probability.</p><p><strong>How it works:</strong> It calculates a probability for each action using the Boltzmann distribution:</p>\\[ P(a|s) = \\frac{\\exp(\\beta Q(s,a))}{\\sum_{a'} \\exp(\\beta Q(s,a'))} \\]<p>The Softmax Temperature parameter \\(\\beta\\) controls the sensitivity to Q-values. Higher \\(\\beta\\) leads to more deterministic (greedy) choices (probabilities concentrate on the max Q-value), while lower \\(\\beta\\) (closer to 0) leads to more uniform, random choices. A value of 0 would make all actions equally likely.</p><p>Unlike ε-Greedy, even non-best actions have a chance of being selected, proportional to their exponentiated Q-value.</p>"
    },
    "random": {
        "title": "Random Exploration",
        "text": "<p><strong>Idea:</strong> Always explore. Choose actions completely randomly, regardless of learned values.</p><p><strong>How it works:</strong> Selects any available action with equal probability (Uniform distribution). This strategy purely explores and does not exploit learned knowledge.</p>\\[ P(a|s) = \\frac{1}{|\\mathcal{A}|} \\]<p>Where \\(|\\mathcal{A}|\\) is the number of possible actions.</p>"
      },
      "greedy": {
        "title": "Greedy Strategy (Pure Exploitation)",
        "text": "<p><strong>Idea:</strong> Always exploit the best-known action based on current estimates.</p><p><strong>How it works:</strong> Always chooses the action with the highest estimated action-value (Q-value, preference, etc.). If multiple actions share the highest value, one is chosen randomly among them.</p>\\[ a_t = \\arg\\max_a Q(s_t,a) \\]<p>This strategy performs no exploration and can easily get stuck in suboptimal policies if the value estimates are not yet accurate.</p>"
      }
  }
} 